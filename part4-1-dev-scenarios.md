# VMware Private AI Foundation 완벽 가이드 - Part 4-1
## 개발 환경별 시나리오 (7장)

> **문서 버전:** 3.1 (v3.1 업데이트)  
> **기반 버전:** VCF 9.0.1, PAIF 9.0, PAIS 2.0.89, DLVM 9.0.1

---

## 7. 개발 환경별 시나리오

PAIF 환경에서 AI 애플리케이션을 구축할 때, 조직의 상황과 요구사항에 따라 세 가지 주요 시나리오 중 하나를 선택할 수 있습니다. 이 장에서는 각 시나리오의 아키텍처와 특징을 비교하여 최적의 선택을 돕습니다.

### 7.1 시나리오 개요 및 선택 가이드

#### 7.1.1 세 가지 시나리오 비교

| 구분 | 시나리오 A: DLVM Only | 시나리오 B: PAIS 전면 활용 | 시나리오 C: 하이브리드 |
|------|----------------------|-------------------------|---------------------|
| **핵심 특징** | 모든 것을 직접 구축 | 관리형 서비스 최대 활용 | PAIS Endpoint + 커스텀 RAG |
| **PAIS 사용** | ❌ 미사용 | ✅ 전면 사용 | ⚡ 부분 사용 (Model Endpoint만) |
| **RAG 구현** | LangChain 등 직접 코딩 | Agent Builder GUI | 직접 구현 |
| **모델 서빙** | vLLM 직접 운영 | Model Endpoint (관리형) | Model Endpoint 활용 |
| **벡터 DB** | pgvector 직접 설정 | Knowledge Base (자동) | 직접 관리 |
| **DevOps 역량** | 🔴 높음 필요 | 🟢 낮음 OK | 🟡 중간 |
| **구축 속도** | 🔴 느림 (수 주) | 🟢 빠름 (수 시간) | 🟡 중간 (수 일) |
| **운영 부담** | 🔴 높음 | 🟢 낮음 | 🟡 중간 |

#### 7.1.2 시나리오 선택 의사결정 플로우

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        시나리오 선택 의사결정 가이드                          │
│                                                                             │
│  [시작] PAIS 라이선스가 있는가?                                              │
│       │                                                                     │
│       ├── NO ──────────────────────────────────────▶ 시나리오 A (DLVM Only) │
│       │                                                                     │
│       └── YES ──▶ RAG 파이프라인 커스터마이징이 필요한가?                    │
│                       │                                                     │
│                       ├── NO ──▶ 빠른 배포가 최우선인가?                    │
│                       │              │                                      │
│                       │              ├── YES ──▶ 시나리오 B (PAIS 전면)     │
│                       │              │           ⭐ 권장                     │
│                       │              │                                      │
│                       │              └── NO ───▶ DevOps 역량이 충분한가?    │
│                       │                              │                      │
│                       │                              ├── YES ──▶ 시나리오 A │
│                       │                              └── NO ───▶ 시나리오 B │
│                       │                                                     │
│                       └── YES ──▶ PAIS Model Endpoint를 활용하고 싶은가?    │
│                                       │                                     │
│                                       ├── YES ──▶ 시나리오 C (하이브리드)   │
│                                       └── NO ───▶ 시나리오 A (DLVM Only)    │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### 7.1.3 시나리오별 적합한 조직/상황

| 시나리오 | 적합한 경우 | 부적합한 경우 |
|----------|------------|--------------|
| **A: DLVM Only** | • R&D/연구소 환경<br>• 완전한 제어권 필요<br>• 기존 MLOps 파이프라인 보유<br>• 특수 모델/프레임워크 사용 | • DevOps 역량 부족<br>• 빠른 time-to-market 필요<br>• 운영 인력 제한 |
| **B: PAIS 전면** | • 일반 기업 AI 도입<br>• 빠른 PoC 필요<br>• 운영 부담 최소화<br>• 표준 RAG 패턴 사용 | • 고급 커스터마이징 필요<br>• 특수 벡터 DB 요구<br>• 기존 RAG 코드 재사용 필수 |
| **C: 하이브리드** | • 커스텀 RAG 로직 필요<br>• 기존 LangChain 코드 활용<br>• 다양한 벡터 DB 연동<br>• 점진적 PAIS 도입 | • 단순한 Q&A 봇<br>• 운영 복잡성 회피 필요 |

---

### 7.2 시나리오 A: DLVM Only (DIY 방식)

#### 7.2.1 개요

**시나리오 A**는 PAIS의 관리형 서비스를 사용하지 않고, DLVM(Deep Learning VM)만으로 전체 AI 스택을 직접 구축하는 방식입니다. 최대한의 유연성과 제어권을 제공하지만, 모든 컴포넌트를 직접 설치, 구성, 운영해야 합니다.

**핵심 구성요소:**

| 구성요소 | 역할 | 직접 관리 항목 |
|---------|------|---------------|
| **DLVM** | GPU 가속 개발 환경 | VM 배포, 리소스 관리 |
| **vLLM** | LLM 서빙 엔진 | 설치, 설정, 모니터링 |
| **pgvector** | 벡터 데이터베이스 | 스키마 설계, 인덱스 관리 |
| **LangChain** | RAG 프레임워크 | 코드 개발, 유지보수 |
| **FastAPI** | API 서버 | 개발, 배포, 운영 |

**장점:**
- ✅ 완전한 커스터마이징 가능
- ✅ 최신 프레임워크/라이브러리 즉시 적용
- ✅ 기존 코드/파이프라인 재사용
- ✅ 벤더 종속성 최소화

**단점:**
- ❌ 높은 DevOps 역량 요구
- ❌ 모든 컴포넌트 직접 운영/모니터링
- ❌ 프로덕션 전환 시 추가 작업 필요
- ❌ 보안/인증 직접 구현

#### 7.2.2 아키텍처

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        시나리오 A: DLVM Only 아키텍처                        │
│                                                                             │
│   [사용자]                                                                   │
│      │                                                                      │
│      │ HTTP Request                                                         │
│      ▼                                                                      │
│   ┌───────────────────────────────────────────────────────────────────────┐ │
│   │                        DLVM (또는 VKS Cluster)                        │ │
│   │                                                                       │ │
│   │   ┌─────────────────┐    ┌─────────────────┐    ┌────────────────┐  │ │
│   │   │   API Server    │    │   RAG Engine    │    │   Vector DB    │  │ │
│   │   │   (FastAPI)     │───▶│   (LangChain)   │───▶│   (pgvector)   │  │ │
│   │   │                 │    │                 │    │                │  │ │
│   │   │ • REST API      │    │ • Retriever     │    │ • 문서 임베딩  │  │ │
│   │   │ • 인증 처리     │    │ • QA Chain      │    │ • 유사도 검색  │  │ │
│   │   │ • 비즈니스 로직 │    │ • 프롬프트 관리 │    │ • 메타데이터   │  │ │
│   │   └─────────────────┘    └────────┬────────┘    └────────────────┘  │ │
│   │                                   │                                  │ │
│   │                                   │ LLM 추론 요청                    │ │
│   │                                   ▼                                  │ │
│   │                          ┌─────────────────┐                        │ │
│   │                          │   vLLM Server   │                        │ │
│   │                          │                 │                        │ │
│   │                          │ • OpenAI 호환   │                        │ │
│   │                          │ • GPU 추론      │                        │ │
│   │                          │ • 배치 처리     │                        │ │
│   │                          └────────┬────────┘                        │ │
│   │                                   │ GPU 액세스                       │ │
│   │                                   ▼                                  │ │
│   │                          ┌─────────────────┐                        │ │
│   │                          │   NVIDIA vGPU   │                        │ │
│   │                          │   (A100 40GB)   │                        │ │
│   │                          └─────────────────┘                        │ │
│   └───────────────────────────────────────────────────────────────────────┘ │
│                                                                             │
│   ┌───────────────────────────────────────────────────────────────────────┐ │
│   │                         공유 인프라 (선택적)                           │ │
│   │  • Harbor: 컨테이너 이미지 저장                                       │ │
│   │  • DSM: pgvector용 PostgreSQL 관리                                    │ │
│   └───────────────────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### 7.2.3 구성 단계 개요

시나리오 A에서 RAG 기반 AI 서비스를 구축하려면 다음 단계를 거칩니다:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    시나리오 A 구성 단계 (DIY)                                │
│                                                                             │
│  Step 1: DLVM 배포                                                          │
│  └─▶ VCF Automation 카탈로그에서 AI Workstation 요청                        │
│      • VM Class 선택 (vgpu-a100-40c 등)                                     │
│      • Software Bundle 선택 (PyTorch 권장)                                  │
│                                                                             │
│  Step 2: 추론 엔진 설치                                                      │
│  └─▶ vLLM 설치 및 모델 서빙 설정                                            │
│      • pip install vllm                                                     │
│      • 모델 다운로드 (Hugging Face, NGC)                                    │
│      • vLLM serve 실행 (systemd 서비스 등록 권장)                           │
│                                                                             │
│  Step 3: 벡터 DB 구성                                                        │
│  └─▶ pgvector 설치 또는 DSM 활용                                            │
│      • PostgreSQL + pgvector 확장                                           │
│      • 테이블/인덱스 생성 (HNSW 또는 IVFFlat)                               │
│                                                                             │
│  Step 4: RAG 파이프라인 구현                                                 │
│  └─▶ LangChain 등 프레임워크로 직접 코딩                                    │
│      • 문서 로딩/청킹                                                       │
│      • 임베딩 및 벡터 저장                                                  │
│      • Retriever + LLM Chain 구성                                           │
│                                                                             │
│  Step 5: API 서버 구현                                                       │
│  └─▶ FastAPI 등으로 REST API 개발                                           │
│      • 인증/인가 직접 구현                                                  │
│      • 비즈니스 로직 통합                                                   │
│                                                                             │
│  Step 6: 프로덕션 배포 (별도 작업)                                           │
│  └─▶ 컨테이너화 + VKS/Helm 배포                                             │
│      • Docker 이미지 빌드                                                   │
│      • Kubernetes 매니페스트 작성                                           │
│      • CI/CD 파이프라인 구축                                                │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### 7.2.4 주요 고려사항

| 항목 | 고려사항 | 권장사항 |
|------|---------|---------|
| **vLLM 버전** | CUDA 버전과 호환성 확인 필요 | DLVM 9.0.1은 CUDA 12.4 기반, vLLM 0.6.x 권장 |
| **모델 다운로드** | Hugging Face 토큰, 라이선스 동의 필요 | Llama 모델은 Meta 라이선스 동의 후 다운로드 |
| **pgvector 인덱스** | 데이터 규모에 따른 선택 | HNSW(빠름, 정확) vs IVFFlat(대용량) |
| **GPU 메모리** | 모델 크기에 따른 요구량 상이 | 8B 모델: 40GB, 70B 모델: 다중 GPU 필요 |
| **운영 모니터링** | 자체 구축 필요 | Prometheus + Grafana, DCGM Exporter |

> **⚠️ 주의:** 시나리오 A는 개발/테스트 환경에서는 유연하지만, 프로덕션 전환 시 컨테이너화, 오케스트레이션, 모니터링 등 상당한 추가 작업이 필요합니다.

---

### 7.3 시나리오 B: PAIS 전면 활용 (권장)

#### 7.3.1 개요

**시나리오 B**는 VMware PAIS(Private AI Services)의 관리형 서비스를 최대한 활용하는 방식입니다. Model Endpoint, Knowledge Base, Agent Builder 등을 통해 코드 작성 없이 RAG 기반 AI 서비스를 빠르게 구축할 수 있습니다.

**핵심 구성요소:**

| 구성요소 | 역할 | PAIS가 제공하는 것 |
|---------|------|-------------------|
| **Model Endpoint** | LLM/Embedding 모델 서빙 | vLLM/Infinity 자동 배포, 스케일링, OpenAI 호환 API |
| **Knowledge Base** | 문서 인덱싱 및 벡터 검색 | 자동 청킹, 임베딩, pgvector 저장, 주기적 갱신 |
| **Agent Builder** | RAG 파이프라인 구성 | GUI 기반 설정, Playground 테스트, REST API 자동 생성 |
| **ML API Gateway** | API 진입점 | 인증/인가, 로드 밸런싱, Rate Limiting |

**장점:**
- ✅ 빠른 구축 (수 시간 내 PoC 가능)
- ✅ 운영 부담 최소화 (VMware가 관리)
- ✅ 자동 스케일링 (VKS 기반)
- ✅ 통합 인증 (VCF OIDC)
- ✅ 프로덕션 준비 완료 상태

**단점:**
- ❌ 커스터마이징 제한 (PAIS가 지원하는 범위 내)
- ❌ 지원 모델/프레임워크 제한
- ❌ PAIS 라이선스 필요
- ❌ 복잡한 RAG 로직 구현 어려움 (예: 멀티홉 추론)

#### 7.3.2 아키텍처

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                       시나리오 B: PAIS 전면 활용 아키텍처                     │
│                                                                             │
│   [사용자/앱]                                                                │
│      │                                                                      │
│      │ HTTPS Request                                                        │
│      ▼                                                                      │
│   ┌───────────────────────────────────────────────────────────────────────┐ │
│   │                    ML API Gateway (PAIS)                              │ │
│   │  • 인증/인가 (OIDC)  • 로드 밸런싱  • OpenAI API 호환                 │ │
│   └───────────────────────────────────────────────────────────────────────┘ │
│                           │                                                 │
│              ┌────────────┴────────────┐                                   │
│              │                         │                                   │
│              ▼                         ▼                                   │
│   ┌─────────────────────┐   ┌─────────────────────┐                       │
│   │  Model Endpoint     │   │  Agent API          │                       │
│   │  (단순 추론)        │   │  (RAG 통합)         │                       │
│   │                     │   │                     │                       │
│   │ • /v1/chat/         │   │ • /v1/agents/       │                       │
│   │   completions       │   │   {name}/chat       │                       │
│   │ • /v1/embeddings    │   │                     │                       │
│   └─────────────────────┘   └──────────┬──────────┘                       │
│                                        │                                   │
│              ┌─────────────────────────┼─────────────────────────┐        │
│              │                         │                         │        │
│              ▼                         ▼                         ▼        │
│   ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐  │
│   │  Completion     │      │  Embedding      │      │  Knowledge      │  │
│   │  Endpoint       │      │  Endpoint       │      │  Base           │  │
│   │  (vLLM)         │      │  (Infinity)     │      │  (pgvector)     │  │
│   │                 │      │                 │      │                 │  │
│   │ • Llama 3.1     │      │ • BGE-Small     │      │ • 자동 청킹     │  │
│   │ • Mistral       │      │ • E5            │      │ • 벡터 임베딩   │  │
│   │ • Qwen          │      │ • CPU 가능 ⭐   │      │ • 주기적 갱신   │  │
│   └────────┬────────┘      └─────────────────┘      └────────┬────────┘  │
│            │ GPU                                              │           │
│            ▼                                                  │           │
│   ┌─────────────────┐                              ┌─────────┴────────┐  │
│   │  NVIDIA vGPU    │                              │  Data Source     │  │
│   │  (VKS Pod)      │                              │  (외부 연결)     │  │
│   └─────────────────┘                              │ • Confluence     │  │
│                                                    │ • SharePoint     │  │
│                                                    │ • Google Drive   │  │
│                                                    │ • S3             │  │
│                                                    └──────────────────┘  │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### 7.3.3 Knowledge Base 개념

**Knowledge Base**는 PAIS가 제공하는 **관리형 RAG 데이터 저장소**입니다.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    Knowledge Base 동작 원리                                  │
│                                                                             │
│   [Data Source]                                                             │
│   (Confluence, SharePoint, S3 등)                                           │
│         │                                                                   │
│         │ ① 문서 수집 (자동/수동)                                           │
│         ▼                                                                   │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │                    PAIS Data Indexing Pipeline                      │  │
│   │                                                                     │  │
│   │   ② 문서 파싱          ③ 청킹               ④ 임베딩              │  │
│   │   (PDF, DOCX, HTML)    (설정된 크기로 분할)  (Embedding Endpoint)   │  │
│   │         │                    │                    │                 │  │
│   │         ▼                    ▼                    ▼                 │  │
│   │   ┌──────────┐         ┌──────────┐         ┌──────────┐          │  │
│   │   │ 원본문서 │   ───▶  │ 텍스트   │   ───▶  │ 벡터     │          │  │
│   │   │          │         │ 청크들   │         │ (384차원)│          │  │
│   │   └──────────┘         └──────────┘         └──────────┘          │  │
│   │                                                   │                 │  │
│   └───────────────────────────────────────────────────┼─────────────────┘  │
│                                                       │                    │
│                                                       │ ⑤ 저장            │
│                                                       ▼                    │
│                                              ┌────────────────┐           │
│                                              │  pgvector DB   │           │
│                                              │  (DSM 관리)    │           │
│                                              └────────────────┘           │
│                                                                             │
│   ⑥ 주기적 갱신 (Daily/Weekly 설정 가능)                                   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

**Knowledge Base 생성 시 설정 항목:**

| 설정 | 설명 | 권장값 |
|------|------|-------|
| **Data Source** | 연결할 문서 저장소 | Confluence, SharePoint, S3 등 |
| **Embedding Model** | 벡터 변환에 사용할 모델 | bge-small-en (영어), e5-small-v2 (다국어) |
| **Chunk Size** | 문서 분할 크기 (토큰) | FAQ: 200-300, 일반: 400-600, 기술문서: 600-1000 |
| **Chunk Overlap** | 청크 간 중복 (토큰) | 50-100 (맥락 유지용) |
| **Indexing Schedule** | 갱신 주기 | Daily (권장), Weekly, Manual |

#### 7.3.4 Agent Builder 개념

**Agent**는 LLM + Knowledge Base + System Prompt를 결합한 **RAG 파이프라인의 캡슐화**입니다.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         Agent 구성 요소                                      │
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │                           Agent                                     │  │
│   │                                                                     │  │
│   │   ┌─────────────────┐                                              │  │
│   │   │ System Prompt   │  "당신은 HR 정책 전문 어시스턴트입니다..."    │  │
│   │   │ (Instructions)  │  규칙, 톤앤매너, 제약 조건 정의               │  │
│   │   └─────────────────┘                                              │  │
│   │            +                                                        │  │
│   │   ┌─────────────────┐                                              │  │
│   │   │ Model Endpoint  │  Llama 3.1 8B Instruct (Completion)          │  │
│   │   │ (LLM)           │  Temperature, Max Tokens 설정                │  │
│   │   └─────────────────┘                                              │  │
│   │            +                                                        │  │
│   │   ┌─────────────────┐                                              │  │
│   │   │ Knowledge Base  │  hr-policy-kb (RAG 데이터 소스)              │  │
│   │   │ (RAG)           │  Similarity Cutoff, Chunk 개수 설정          │  │
│   │   └─────────────────┘                                              │  │
│   │            +                                                        │  │
│   │   ┌─────────────────┐                                              │  │
│   │   │ Session Config  │  Chat History 길이, Session 만료 시간        │  │
│   │   │ (대화 관리)     │  멀티턴 대화 지원                            │  │
│   │   └─────────────────┘                                              │  │
│   │                                                                     │  │
│   │   ═══════════════════════════════════════════════════════════════  │  │
│   │                              ▼                                      │  │
│   │   ┌─────────────────────────────────────────────────────────────┐  │  │
│   │   │              REST API 자동 생성                              │  │  │
│   │   │              POST /v1/agents/{agent-name}/chat               │  │  │
│   │   └─────────────────────────────────────────────────────────────┘  │  │
│   │                                                                     │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

**Agent 생성 시 주요 설정:**

| 설정 | 설명 | 권장값 |
|------|------|-------|
| **Model Endpoint** | 응답 생성에 사용할 LLM | llama-3-1-8b-instruct |
| **Temperature** | 응답의 창의성/일관성 (0.0~2.0) | 정확성 중요: 0.3, 일반: 0.7 |
| **Max Tokens** | 최대 응답 길이 | 256~1024 |
| **Knowledge Base** | 연결할 KB | 용도별 KB 선택 |
| **Similarity Cutoff** | 관련성 임계값 (0.0~1.0) | 0.6~0.8 (낮으면 노이즈, 높으면 누락) |
| **Number of Chunks** | 검색할 문서 개수 | 3~7개 |
| **Chat History Length** | 대화 맥락 유지 메시지 수 | 5~15 |

#### 7.3.5 구성 단계 개요

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    시나리오 B 구성 단계 (PAIS)                               │
│                                                                             │
│  Step 1: Model Endpoint 생성 (MLOps Engineer)                               │
│  └─▶ PAIS UI에서 Completion/Embedding Endpoint 배포                        │
│      • Harbor에서 모델 선택                                                 │
│      • VM Class, Replicas 설정                                              │
│      • 자동 스케일링 옵션                                                   │
│                                                                             │
│  Step 2: Data Source 연결 (Data Scientist)                                  │
│  └─▶ 문서 저장소 연결 설정                                                  │
│      • Confluence, SharePoint, S3 등 선택                                   │
│      • 인증 정보 입력                                                       │
│                                                                             │
│  Step 3: Knowledge Base 생성 (Data Scientist)                               │
│  └─▶ 문서 인덱싱 설정                                                       │
│      • Data Source 선택                                                     │
│      • Embedding Model, Chunk 설정                                          │
│      • 갱신 주기 설정                                                       │
│                                                                             │
│  Step 4: Agent 생성 (Data Scientist)                                        │
│  └─▶ RAG 파이프라인 구성                                                    │
│      • Model Endpoint + Knowledge Base 연결                                 │
│      • System Prompt 작성                                                   │
│      • 파라미터 튜닝                                                        │
│                                                                             │
│  Step 5: Playground 테스트                                                  │
│  └─▶ 즉시 대화형 테스트 가능                                                │
│      • 프롬프트 반복 개선                                                   │
│      • 샘플 코드 복사                                                       │
│                                                                             │
│  Step 6: 앱 연동 (App Developer)                                            │
│  └─▶ Agent API URL로 앱에서 호출                                            │
│      • 이미 프로덕션 준비 완료 상태                                         │
│      • 추가 인프라 작업 불필요                                              │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

> **💡 핵심 차이:** 시나리오 A에서는 Step 6(프로덕션 배포)이 별도의 큰 작업이지만, 시나리오 B에서는 Agent 생성 시점에 이미 프로덕션 준비가 완료됩니다.

---

### 7.4 시나리오 C: 하이브리드

#### 7.4.1 개요

**시나리오 C**는 PAIS의 Model Endpoint만 활용하고, RAG 파이프라인은 직접 구현하는 방식입니다. 모델 서빙의 편의성과 RAG 커스터마이징의 유연성을 동시에 얻을 수 있습니다.

**핵심 구성요소:**

| 구성요소 | 제공 방식 | 설명 |
|---------|----------|------|
| **Model Endpoint** | PAIS 관리형 | LLM/Embedding 서빙을 PAIS에 위임 |
| **RAG Pipeline** | 직접 구현 | LangChain 등으로 커스텀 로직 |
| **Vector DB** | 선택적 | pgvector, Milvus, Qdrant, Chroma 등 |

**장점:**
- ✅ 모델 서빙 운영 부담 감소 (PAIS 위임)
- ✅ RAG 로직 완전한 커스터마이징
- ✅ 다양한 벡터 DB 선택 가능
- ✅ 기존 RAG 코드 재사용
- ✅ 점진적 PAIS 도입 가능

**단점:**
- ❌ RAG 인프라 직접 운영 필요
- ❌ PAIS + 커스텀 양쪽 관리
- ❌ Knowledge Base/Agent 기능 미사용

#### 7.4.2 아키텍처

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                       시나리오 C: 하이브리드 아키텍처                        │
│                                                                             │
│   [사용자]                                                                   │
│      │                                                                      │
│      │ HTTPS Request                                                        │
│      ▼                                                                      │
│   ┌───────────────────────────────────────────────────────────────────────┐ │
│   │                    Custom Application (DLVM 또는 VKS)                 │ │
│   │                                                                       │ │
│   │   ┌─────────────────┐    ┌─────────────────────────────────────────┐ │ │
│   │   │   API Server    │    │        Custom RAG Engine                │ │ │
│   │   │   (FastAPI)     │───▶│        (LangChain)                      │ │ │
│   │   │                 │    │                                         │ │ │
│   │   │ • REST API      │    │  • Multi-Query Retriever               │ │ │
│   │   │ • 인증 처리     │    │  • Contextual Compression               │ │ │
│   │   │ • 비즈니스 로직 │    │  • 커스텀 프롬프트                      │ │ │
│   │   │                 │    │  • 복잡한 체인 구성                     │ │ │
│   │   └─────────────────┘    └───────────┬─────────────────────────────┘ │ │
│   │                                      │                               │ │
│   └──────────────────────────────────────┼───────────────────────────────┘ │
│                                          │                                 │
│                    ┌─────────────────────┼─────────────────────┐          │
│                    │                     │                     │          │
│                    ▼                     ▼                     ▼          │
│   ┌─────────────────────┐  ┌─────────────────────┐  ┌─────────────────┐  │
│   │   Custom Vector DB  │  │  PAIS Completion    │  │  PAIS Embedding │  │
│   │                     │  │  Endpoint           │  │  Endpoint       │  │
│   │ • pgvector (DSM)    │  │                     │  │                 │  │
│   │ • Milvus            │  │ • vLLM + Llama 3.1  │  │ • Infinity      │  │
│   │ • Qdrant            │  │ • OpenAI 호환 API   │  │ • BGE-Small     │  │
│   │ • Chroma (임베디드) │  │ • 자동 스케일링     │  │ • OpenAI 호환   │  │
│   │                     │  │                     │  │                 │  │
│   └─────────────────────┘  └─────────────────────┘  └─────────────────┘  │
│         (직접 관리)               (PAIS 관리형)          (PAIS 관리형)    │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### 7.4.3 시나리오 C 적합 사례

| 사례 | 설명 | 이유 |
|------|------|------|
| **기존 RAG 코드 보유** | 이미 LangChain 기반 RAG 구현체가 있음 | 코드 재작성 없이 모델만 PAIS로 전환 |
| **고급 검색 로직** | Multi-hop, Self-RAG 등 복잡한 패턴 | PAIS Agent로는 구현 불가 |
| **특수 벡터 DB** | Milvus, Qdrant 등 특정 DB 필요 | PAIS는 pgvector만 지원 |
| **점진적 전환** | 기존 DIY에서 PAIS로 단계적 이전 | Model Endpoint부터 시작 |

#### 7.4.4 PAIS Endpoint 활용 방식

시나리오 C에서는 PAIS Model Endpoint를 **OpenAI 호환 API**로 호출합니다.

```python
# PAIS Completion Endpoint (OpenAI 호환)
llm = ChatOpenAI(
    openai_api_base = "https://pais.company.com/v1",
    openai_api_key  = "<PAIS_TOKEN>",
    model_name      = "llama-3-1-8b-instruct"   # Endpoint 이름
)

# PAIS Embedding Endpoint (OpenAI 호환)
embeddings = OpenAIEmbeddings(
    openai_api_base = "https://pais.company.com/v1",
    openai_api_key  = "<PAIS_TOKEN>",
    model           = "bge-small-en"            # Endpoint 이름
)

# 이후 일반적인 LangChain 사용과 동일
vectorstore = PGVector(..., embedding_function=embeddings)
qa_chain = RetrievalQA.from_chain_type(llm=llm, ...)
```

> **💡 핵심 포인트:** PAIS Model Endpoint는 OpenAI API와 호환되므로, 기존 OpenAI 기반 코드에서 Base URL과 Model 이름만 변경하면 됩니다.

---

### 7.5 시나리오별 비교 요약

#### 7.5.1 종합 비교표

| 비교 항목 | 시나리오 A (DLVM Only) | 시나리오 B (PAIS 전면) | 시나리오 C (하이브리드) |
|----------|----------------------|---------------------|---------------------|
| **초기 구축 시간** | 수 일 ~ 수 주 | 수 시간 ~ 1일 | 수 일 |
| **DevOps 역량** | 고급 필수 | 기초 OK | 중급 |
| **운영 부담** | 높음 (전체 스택) | 낮음 (PAIS 관리) | 중간 (RAG만) |
| **커스터마이징** | 무제한 | 제한적 | RAG는 자유 |
| **프로덕션 전환** | 별도 작업 필요 | 즉시 가능 | 별도 작업 필요 |
| **비용 예측** | VM 단위 | Endpoint 단위 | 혼합 |
| **벤더 종속성** | 최소 | 중간 (PAIS) | 낮음 |

#### 7.5.2 의사결정 요약

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         시나리오 선택 요약                                   │
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │                                                                     │  │
│   │   "빠르게 시작하고 싶다"                    ──▶  시나리오 B ⭐       │  │
│   │                                                                     │  │
│   │   "완전한 제어권이 필요하다"                ──▶  시나리오 A          │  │
│   │                                                                     │  │
│   │   "기존 RAG 코드를 재사용하고 싶다"         ──▶  시나리오 C          │  │
│   │                                                                     │  │
│   │   "PAIS 라이선스가 없다"                    ──▶  시나리오 A          │  │
│   │                                                                     │  │
│   │   "운영 인력이 제한적이다"                  ──▶  시나리오 B          │  │
│   │                                                                     │  │
│   │   "복잡한 RAG 로직이 필요하다"              ──▶  시나리오 A 또는 C   │  │
│   │                                                                     │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│   💡 대부분의 엔터프라이즈 환경에서는 **시나리오 B**를 권장합니다.          │
│      빠른 PoC 후 필요시 시나리오 C로 전환하는 것도 좋은 전략입니다.         │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

**[Part 4-1 완료]**

다음 Part 4-2에서는 다음 내용을 다룹니다:
- 8장: AI 앱 개발 이해
  - 일반 웹앱 vs AI 앱 구조 비교
  - AI 앱의 3-Tier 아키텍처
  - PAIS API 연동 개념
  - 배포 아키텍처
  - 운영 고려사항
